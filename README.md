# Learning-VLA
A hands-on lab for learning and implementing Visual Language Models (CLIP, BLIP-2, VLMs, VLAs) step by step.
This repository is my personal learning playground for exploring Vision-Language Models (VLMs) and eventually building towards Vision-Language-Action (VLA) systems.

I’ll start small with CLIP (image-text alignment), then move to BLIP-2 (image captioning), OwlViT (open-vocabulary detection), and finally explore VLM integration and VLAs.

Each model implementation will include:

📜 Notes (paper summaries & concepts)

💻 Code (demo scripts + Jupyter notebooks)

🖼️ Sample tests (laptop, flower, etc.)

The goal: build intuition by coding and documenting everything step by step.
